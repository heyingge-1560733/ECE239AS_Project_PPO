# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ehWbCaNeeO34Zq9lxBWmvMNiIIlWQzc6
"""

import copy
import pdb
import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn

class PPOClassical:
  def __init__(self, config):
    self.config = config 
    self.memory = config.memory(config.update_every, config.num_env, config.env, config.device)

    self.device = config.device
    
    self.gamma = config.gamma 
    self.epsilon = config.epsilon
    self.beta = config.entropy_beta

    self.model = config.model(config).to(self.device)
    self.old_model = config.model(config).to(self.device)
    self.old_model.load_state_dict(self.model.state_dict())
    
    self.optimiser = optim.Adam(self.model.parameters(), lr=config.lr, eps=1e-5)
  
  def act(self, x):
    raise NotImplemented
  
  def add_to_mem(self, state, action, reward, log_prob, done):
    raise NotImplemented

  def learn(self, num_learn):
    raise NotImplemented
    
class PPOPixel:
  def __init__(self, config):
    
    self.config = config 
    self.memory = config.memory(config.update_every, config.num_env, config.env, config.device)

    self.device = config.device
    
    self.gamma = config.gamma 
    self.epsilon = config.epsilon
    self.beta = config.entropy_beta

    self.model = config.model(config).to(self.device)
    self.old_model = config.model(config).to(self.device)
    self.old_model.load_state_dict(self.model.state_dict())
    
    self.optimiser = optim.Adam(self.model.parameters(), lr=config.lr, eps=1e-5)

  def act(self, x):
    x = x.to(self.device)
    return self.old_model.act(x)

  def add_to_mem(self, s, a, r, log_p, done):
    self.memory.add(s, a, r, log_p, done)

  def learn(self, num_epoch, last_value, next_done):
    # compute the discounted returns using rewards collected from environments
    self.memory.calculate_discounted_returns(last_value, next_done)
    
    for i in range(num_epoch):
      # itterate over mini_batches
      for mini_batch_idx in self.memory.get_mini_batch_idxs(mini_batch_size=100):

        # Grab sample from memory
        prev_S, prev_A, prev_log_P, discounted_R = self.memory.sample(mini_batch_idx)

        V = self.old_model.get_values(prev_S).reshape(-1)

        # calculate normalized advantage 
        adv = discounted_R - prev_V
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)

        # find ratios
        _, log_P, _, entropy = self.model.act(prev_S, prev_A)
        r = torch.exp(log_P - prev_log_P.detach())
        
        # find KL divergence
        approx_kl = (prev_log_P - log_P).mean()
        
        # calculate surrogates
        surrogate_1 = r * adv
        surrogate_2 = torch.clamp(r, 1-self.epsilon, 1+self.epsilon) * adv

        # Calculate losses
        value_loss =  (discounted_R - V).pow(2).mean()
        pg_loss = -torch.min(surrogate_1, surrogate_2).mean()
        entropy_loss = entropy.mean()

        loss = pg_loss + value_loss - self.beta*entropy_loss

        # calculate gradient
        self.optimiser.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        self.optimiser.step()

        if torch.abs(approx_kl) > 0.03:
          break
      
      self.old_model.load_state_dict(self.model.state_dict())

    return value_loss, pg_loss, approx_kl, entropy_loss

